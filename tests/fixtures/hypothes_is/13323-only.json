[
  {
    "@context": "http://www.w3.org/ns/anno.jsonld",
    "id": "disqus-import:2537434990",
    "type": "Annotation",
    "created": "2016-02-26T16:20:27Z",
    "modified": "2016-02-26T16:20:27Z",
    "creator": "acct:disqus-import@test.elifesciences.org",
    "motivation": "commenting",
    "body": [
      {
        "type": "TextualBody",
        "value": "_Legacy comment by Leila Agha and Danielle Li_\n\nIn their paper, Fang, Bowen, and Casadevall argue that NIH peer review scores are weakly predictive of research productivity for grant-supported projects scoring in the top 20 percent. Fang et al. suggest that the findings reported by Li and Agha (2015) are driven by the inclusion of more poorly scored grants that were nonetheless funded. More specifically, Fang et al. argue that there is a markedly weaker relationship between peer review scores and grant productivity for projects scored between 3 and 20 than in the full sample considered by Li and Agha (2015). We first demonstrate that this conclusion is not supported by the data. Restricting our sample to only grants scored in the interval \\[3, 20\\] suggested by Fang et al., we estimate poisson regressions that parallel the specifications in Li and Agha (2015).\n\nIn the restricted sample, we find that the estimated relationship between scores and outcomes is slightly larger in magnitude to the findings reported in Li and Agha\u2019s original Science paper, and strongly statistically significant. In the baseline model specification without control variables, we find a one point worse peer review score is associated with 2.14% fewer citations in the restricted sample of funded grants scored between \\[3,20\\], as compared to a 2.03% decrease in the full sample of funded grants reported in Li and Agha (2015). In the accompanying table, we demonstrate that the relationship between scores and bibliometric outcomes persists in the \\[3,20\\]  \nscore range for all model specifications reported in the original paper.\n\nThese relationships can also be inferred graphically from Agha and Li (2015) Figures 1 and 2, which illustrate that both publications and citations fall as peer review scores become weaker, including over the 3-20 score range. Fang et al.\u2019s Figure 2 illustrates a similar point, with a majority of applications falling into the top 50% of the productivity distribution for grants scored at every percentile below 10, and a majority  \nfalling into the bottom 50% of the productivity distribution for grants scored at every percentile between 10 and 20.\n\nPutting that aside, Fang et al.\u2019s article points out that there is wide variation in NIH grant outcomes that is not perfectly predicted by percentile scores. This fact is also supported by the results reported in Li and Agha (2015), including the scatterplots in  \nFigure 1 of the paper, which illustrate that there is substantial variation in outcomes, conditional on score: many highly scored grants produce few citations and vice versa. NIH review committees are not perfect forecasters of future research performance. Of course, this implies that many worthy projects may go unfunded, particularly with paylines set at historically low levels.\n\nA note of caution is necessary in interpreting the correlation between committee scores and grant First, we rely on grant acknowledgements in published articles to link applications to future research output. While this is the best available measure of grant output, it is imperfect, since it relies on PI compliance. Compliance appears to have  \nimproved over our study period, with the proportion of grants with no associated  \npublications falling by almost half between 1980 and 2008, but even in recent  \nyears, we will likely fail to capture some associated research output. Second, we only link publications that are published within 5 years after the application was funded to ensure that publications were indeed related to the grant; however, some research may take longer to accrue publications. These data limitations will tend to attenuate  \nthe measured relationship between scores and outcomes, suggesting that with  \nbetter linkages, we may find an even stronger correlation. These factors could  \nalso contribute to Fang et al.\u2019s observation that a substantial fraction of grants receiving a score in the top 1% have no accrued citations.\n\nFang et al. conclude that a lottery may be a more appropriate way to award funding within the top 20% given the substantial variation in grant outcomes not perfectly predicted by peer review scores. From a policymaker\u2019s perspective, this is a risky proposition. Suppose that only half of grants scoring in the top 20% could be funded. In our sample, grants scored in the top 10 percent accrue 12% more total citations than the expected performance of a randomly selected set of grants in the top 20%.  \nHence, Fang et al.\u2019s proposed policy of random assignment could substantially  \nreduce the total productivity of public research funding.\n\nOur reported results demonstrate that peer review scores are a significant predictor of research outcomes. This relationship between scores and outcomes persists even after accounting for many additional factors in our statistical models, including PI\u2019s publication history, citation history, past grant performance, institutional affiliation,  \ncareer stage, and field of study. There is currently no evidence that other systems of allocating scarce NIH funds that do not rely on expert peer review could outperform the current allocation mechanism.\n\n_Notes for accompanying table:_   \nEach reported figure is the coefficient on scores from a single Poisson regression of grant outcomes on NIH peer review scores; standard errors are reported in parentheses. The sample includes all NIH-funded R01 grants from 1980-2008 that were assigned a percentile score greater than or equal to 3 and less than or equal to 20. We restrict to new and competing renewal applications that received study section percentile scores. The actual sample size used per regression depends on the number of non-zero observations for the dependent variable. The independent variable is  \nthe percentile score: for each funded grant, this refers to the percent of other applications to the same study section - year that received a better study section priority score (lower percentiles correspond to better rankings). Future citations refers to the total number of citations, to 2013, that accrue to all publications that acknowledge funding from a given grant. Future publications refers to the total number of such publications. Subject-year controls refer to study section by fiscal year fixed effects, as well as NIH Institute fixed effects. PI publication history includes controls for number of past publications, number of past citations, and number of past hit publications. PI career characteristics include controls for degrees and experience (time since highest degree). PI grant history controls for number of prior R01s and non R01 NIH funding. PI institution and demographics control for the rank of the PI's instiution, as well as gender and some ethnicity controls. Standard errors are clustered at the study section - year level. \\* denotes statistical significance at the 5% level. \\*\\* at the 1% level. \\*\\*\\* at the 0.1% level. See Supporting Online Material for more details.",
        "format": "text/markdown"
      }
    ],
    "target": "https://elifesciences.org/articles/13323"
  },
  {
    "@context": "http://www.w3.org/ns/anno.jsonld",
    "id": "disqus-import:2541162796",
    "type": "Annotation",
    "created": "2016-02-28T08:11:03Z",
    "modified": "2016-02-28T08:11:03Z",
    "creator": "acct:disqus-import@test.elifesciences.org",
    "motivation": "commenting",
    "body": [
      {
        "type": "TextualBody",
        "value": "_Legacy comment by Denis Sunko_\n\nThe lack of correlation is a consequence of the high rejection rate and has nothing to do with peer review as such. Any given property is randomly distributed in the tail end of a distribution. If the high rejection rate is driven by extraneous factors, peer review necessarily reduces to a lottery. This paper highlights the simple fact that the only statistically legitimate use of peer review is to eliminate flawed applications, not to rank those that are good enough. The greater the percentage of the latter that are rejected, the greater the lottery.",
        "format": "text/markdown"
      }
    ],
    "target": "https://elifesciences.org/articles/13323"
  },
  {
    "@context": "http://www.w3.org/ns/anno.jsonld",
    "id": "disqus-import:2541909438",
    "type": "Annotation",
    "created": "2016-02-28T19:37:08Z",
    "modified": "2016-02-28T19:37:08Z",
    "creator": "acct:disqus-import@test.elifesciences.org",
    "motivation": "replying",
    "body": [
      {
        "type": "TextualBody",
        "value": "_Legacy comment by Ferric Fang_\n\nWe thank Drs. Agha and Li for their comment. They report a statistically significant correlation between percentile score and productivity for grants in the 3-20 percentile range, while we have shown that the productivity of grants in this range is not significantly different between adjacent cohorts. These findings are not incompatible. The essential point is that the difference in the median productivity of grants stratified by percentile score is extremely small (fewer than 10 citations and about one-eighth of a publication in five  \nyears per percentile score increment), whereas the variation among grants with any given percentile score is substantial. Moreover, as discussed in our paper, even this tiny difference may result from factors other then the predictive ability of peer review. The question remains whether the great effort and expense invested in stratifying meritorious applications is warranted by  \nthe very modest (if any) benefit, given that the process also has considerable shortcomings, such as the potential for reviewer bias.\n\nWhere we agree with Drs. Agha and Li is that \u2018There is substantial variation in outcomes\u2026 many highly scored grants produce few citations and vice versa. NIH review committees are not perfect forecasters of future research performance. Of course, this implies that many worthy projects may go unfunded, particularly with paylines set at historically low levels.\u2019 However, their statement that \u2018There is currently no evidence that other systems of allocating scarce  \nNIH funds that do not rely on expert peer review could outperform the current allocation mechanism\u2019 is true only because the NIH has not evaluated alternative approaches. The logical fallacy in such arguments has been known since antiquity as argumentum ad ignorantiam, or the argument from ignorance-- i.e., the absence of evidence is not evidence for its absence. One might just as easily say that the UK and New Zealand use criteria other than priority scores to allocate funding among meritorious applications, and there is no evidence that these approaches produce inferior outcomes.\n\nF.C. Fang  \nA. Bowen  \nA. Casadevall",
        "format": "text/markdown"
      }
    ],
    "target": "disqus-import:2537434990"
  }
]
